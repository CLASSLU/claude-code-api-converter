from flask import Flask, request, jsonify
import os
import time
import logging
from logging.handlers import RotatingFileHandler
from converter_class import AnthropicToOpenAIConverter
# from smart_converter_fix import SmartConverter  # å·²ç¦ç”¨æ™ºèƒ½ä¿®å¤
from config_manager import ConfigManager
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
import random
import socket
import sys
import psutil
import uuid
import hashlib
import json
from threading import Lock

# Windowså…¼å®¹æ€§å¤„ç†
if sys.platform == 'win32':
    import msvcrt
else:
    import fcntl

# å•å®ä¾‹ä¿æŠ¤æœºåˆ¶
class SingleInstanceChecker:
    """ç¡®ä¿åªæœ‰ä¸€ä¸ªå®ä¾‹è¿è¡Œ"""
    
    def __init__(self, port=8080):
        self.port = port
        self.lock_file = f"api_server_{port}.lock"
        self.lock_fd = None
        
    def check_port_occupied(self):
        """æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.settimeout(1)
                result = s.connect_ex(('localhost', self.port))
                return result == 0
        except:
            return False
    
    def get_existing_processes(self):
        """è·å–ç°æœ‰çš„api_serverè¿›ç¨‹"""
        current_pid = os.getpid()
        existing_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['pid'] == current_pid:
                    continue
                    
                cmdline = proc.info.get('cmdline', [])
                if cmdline and any('api_server.py' in str(arg) for arg in cmdline):
                    existing_processes.append(proc.info['pid'])
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        return existing_processes
    
    def create_lock_file(self):
        """åˆ›å»ºé”æ–‡ä»¶"""
        try:
            self.lock_fd = open(self.lock_file, 'w')
            self.lock_fd.write(str(os.getpid()))
            self.lock_fd.flush()
            
            # åœ¨Windowsä¸Šä½¿ç”¨æ–‡ä»¶é”å®š
            if sys.platform == 'win32':
                import msvcrt
                msvcrt.locking(self.lock_fd.fileno(), msvcrt.LK_NBLCK, 1)
            else:
                fcntl.flock(self.lock_fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                
            return True
        except (IOError, OSError):
            if self.lock_fd:
                self.lock_fd.close()
                self.lock_fd = None
            return False
    
    def cleanup_lock_file(self):
        """æ¸…ç†é”æ–‡ä»¶"""
        try:
            if self.lock_fd:
                self.lock_fd.close()
                self.lock_fd = None
            if os.path.exists(self.lock_file):
                os.remove(self.lock_file)
        except:
            pass
    
    def check_single_instance(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå•ä¸€å®ä¾‹"""
        # æ£€æŸ¥ç«¯å£å ç”¨
        if self.check_port_occupied():
            print(f"ç«¯å£ {self.port} å·²è¢«å ç”¨ï¼")
            existing_processes = self.get_existing_processes()
            if existing_processes:
                print(f"å‘ç°ç°æœ‰çš„APIæœåŠ¡å™¨è¿›ç¨‹: {existing_processes}")
                print("è¯·å…ˆç»ˆæ­¢ç°æœ‰è¿›ç¨‹åå†å¯åŠ¨æ–°å®ä¾‹")
            return False
        
        # æ£€æŸ¥ç°æœ‰è¿›ç¨‹
        existing_processes = self.get_existing_processes()
        if existing_processes:
            print(f"å‘ç°ç°æœ‰çš„APIæœåŠ¡å™¨è¿›ç¨‹: {existing_processes}")
            print("æ­£åœ¨ç»ˆæ­¢ç°æœ‰è¿›ç¨‹...")
            
            for pid in existing_processes:
                try:
                    os.kill(pid, 9)  # å¼ºåˆ¶ç»ˆæ­¢
                    print(f"å·²ç»ˆæ­¢è¿›ç¨‹ {pid}")
                except:
                    try:
                        proc = psutil.Process(pid)
                        proc.terminate()
                        print(f"å·²ç»ˆæ­¢è¿›ç¨‹ {pid}")
                    except:
                        print(f"æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {pid}")
            
            # ç­‰å¾…è¿›ç¨‹å®Œå…¨é€€å‡º
            time.sleep(2)
            
            # å†æ¬¡æ£€æŸ¥
            remaining = self.get_existing_processes()
            if remaining:
                print(f"ä»æœ‰è¿›ç¨‹è¿è¡Œ: {remaining}")
                return False
        
        # åˆ›å»ºé”æ–‡ä»¶
        if not self.create_lock_file():
            print("æ— æ³•åˆ›å»ºé”æ–‡ä»¶ï¼Œå¯èƒ½å·²æœ‰å®ä¾‹åœ¨è¿è¡Œ")
            return False
        
        print("å•å®ä¾‹æ£€æŸ¥é€šè¿‡")
        return True

# å…¨å±€å•å®ä¾‹æ£€æŸ¥å™¨
instance_checker = None

def cleanup_on_exit():
    """ç¨‹åºé€€å‡ºæ—¶æ¸…ç†èµ„æº"""
    global instance_checker
    if instance_checker:
        instance_checker.cleanup_lock_file()

import atexit
atexit.register(cleanup_on_exit)

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # åˆ›å»ºæ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¸¦è½®è½¬ï¼Œæœ€å¤§10MBï¼Œä¿ç•™5ä¸ªå¤‡ä»½ï¼‰
    file_handler = RotatingFileHandler(
        'api_server.log', 
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    
    # æ·»åŠ å¤„ç†å™¨
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    return root_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logging()

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()
app = Flask(__name__)
converter = AnthropicToOpenAIConverter(config_manager)

# ä»é…ç½®ç®¡ç†å™¨è·å–é…ç½®å¹¶ç¼“å­˜
_openai_config = config_manager.get_openai_config()
_server_config = config_manager.get_server_config()

def get_openai_config():
    """è·å–OpenAIé…ç½®ï¼ˆç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    return _openai_config

def get_server_config():
    """è·å–æœåŠ¡å™¨é…ç½®ï¼ˆç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    return _server_config

def refresh_configs():
    """åˆ·æ–°é…ç½®ç¼“å­˜"""
    global _openai_config, _server_config
    _openai_config = config_manager.get_openai_config()
    _server_config = config_manager.get_server_config()

# åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„session
def create_session_with_retry():
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=0,  # æ€»é‡è¯•æ¬¡æ•°
        status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è¯•çš„çŠ¶æ€ç 
        allowed_methods=["HEAD", "GET", "POST"],  # å…è®¸é‡è¯•çš„æ–¹æ³• (æ–°ç‰ˆæœ¬å‚æ•°å)
        backoff_factor=1,  # é‡è¯•é—´éš”é€’å¢å› å­
        raise_on_status=False  # é‡è¯•åä»ç„¶å¤±è´¥æ—¶ä¸æŠ›å‡ºå¼‚å¸¸
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

# åˆ›å»ºå…¨å±€session
http_session = create_session_with_retry()

# è¯·æ±‚å»é‡æœºåˆ¶
class RequestDeduplicator:
    """è¯·æ±‚å»é‡å™¨ï¼Œé˜²æ­¢Claude Codeé‡å¤è¯·æ±‚"""
    
    def __init__(self, cache_duration=30):
        self.cache = {}
        self.cache_duration = cache_duration  # ç¼“å­˜æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        self.lock = Lock()
        
    def _generate_request_hash(self, request_data):
        """ç”Ÿæˆæ›´æ™ºèƒ½çš„è¯·æ±‚å“ˆå¸Œå€¼"""
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæå–æ¶ˆæ¯çš„ä¸»è¦æ–‡æœ¬å†…å®¹ï¼Œå»é™¤åŠ¨æ€å­—æ®µ
        messages = request_data.get('messages', [])
        processed_messages = []

        for msg in messages:
            if msg.get('role') == 'user':
                # åªä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œå»é™¤åŠ¨æ€å­—æ®µ
                content = msg.get('content', '')
                if isinstance(content, list):
                    text_parts = []
                    for item in content:
                        if item.get('type') == 'text':
                            text_parts.append(item.get('text', ''))
                    processed_messages.append({
                        'role': 'user',
                        'content': ''.join(text_parts)
                    })
                else:
                    processed_messages.append({
                        'role': 'user',
                        'content': content
                    })

        # ä½¿ç”¨æ›´ç²¾ç®€çš„å…³é”®å­—æ®µ
        key_fields = {
            'model': request_data.get('model'),
            'messages': processed_messages,
            'tools': request_data.get('tools')  # å·¥å…·å®šä¹‰ä¹Ÿå½±å“å“åº”
        }

        # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶ç”Ÿæˆå“ˆå¸Œ
        request_str = json.dumps(key_fields, sort_keys=True, separators=(',', ':'))
        request_hash = hashlib.md5(request_str.encode()).hexdigest()

        # ğŸ”¥ è°ƒè¯•æ—¥å¿—ï¼šç›‘æ§å“ˆå¸Œç”Ÿæˆ
        logger.debug(f"ğŸ” ç”Ÿæˆè¯·æ±‚å“ˆå¸Œ: {request_hash[:8]}... (æ¶ˆæ¯æ•°: {len(processed_messages)})")

        return request_hash
    
    def is_duplicate_request(self, request_data):
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤è¯·æ±‚"""
        request_hash = self._generate_request_hash(request_data)
        current_time = time.time()

        with self.lock:
            if request_hash in self.cache:
                cached_time, cached_response = self.cache[request_hash]

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if current_time - cached_time < self.cache_duration:
                    cache_age = current_time - cached_time
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤è¯·æ±‚ï¼Œä½¿ç”¨ç¼“å­˜å“åº” (å“ˆå¸Œ: {request_hash[:8]}..., ç¼“å­˜å¹´é¾„: {cache_age:.1f}ç§’)")
                    return True, cached_response
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                    logger.debug(f"ğŸ• ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤æ—§ç¼“å­˜ (å“ˆå¸Œ: {request_hash[:8]}...)")
                    del self.cache[request_hash]
                    logger.info(f"ğŸ” å½“å‰ç¼“å­˜æ¡ç›®æ•°: {len(self.cache)}")

        # ğŸ”¥ ç›‘æ§æ–°å¢è¯·æ±‚
        logger.info(f"ğŸ†• æ–°è¯·æ±‚æ£€æµ‹ (å“ˆå¸Œ: {request_hash[:8]}..., å½“å‰ç¼“å­˜: {len(self.cache)} æ¡)")

        return False, None
    
    def cache_response(self, request_data, response):
        """ç¼“å­˜å“åº”"""
        request_hash = self._generate_request_hash(request_data)
        current_time = time.time()
        
        with self.lock:
            self.cache[request_hash] = (current_time, response)
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            expired_hashes = []
            for hash_key, (cached_time, _) in self.cache.items():
                if current_time - cached_time >= self.cache_duration:
                    expired_hashes.append(hash_key)
            
            for hash_key in expired_hashes:
                del self.cache[hash_key]
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
        logger.info("ğŸ”„ è¯·æ±‚å»é‡ç¼“å­˜å·²æ¸…ç©º")

# å…¨å±€è¯·æ±‚å»é‡å™¨ - ğŸ”¥ å¢åŠ ç¼“å­˜æ—¶é—´åˆ°5åˆ†é’Ÿ
request_deduplicator = RequestDeduplicator(cache_duration=300)

def is_rate_limit_error(response):
    """æ£€æµ‹æ˜¯å¦ä¸ºé™æµé”™è¯¯"""
    if response.status_code == 429:
        return True
    
    response_text = response.text.lower()
    rate_limit_indicators = [
        'tpm', 'rpm', 'rate limit', 'too many requests',
        'rate_limit_exceeded', 'rate limited', 'quota exceeded'
    ]
    
    return any(indicator in response_text for indicator in rate_limit_indicators)

def handle_rate_limit_with_backoff(api_call_func, max_retries=3, request_id=None):
    """
    æ™ºèƒ½é™æµå¤„ç†ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿ç®—æ³•è‡ªåŠ¨é‡è¯•
    
    Args:
        api_call_func: APIè°ƒç”¨å‡½æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        request_id: è¯·æ±‚IDç”¨äºæ—¥å¿—è·Ÿè¸ª
    
    Returns:
        APIå“åº”æˆ–é”™è¯¯å“åº”
    """
    req_prefix = f"[REQ:{request_id}]" if request_id else "[RETRY]"
    
    for attempt in range(max_retries + 1):
        try:
            response = api_call_func()
            
            # å¦‚æœæˆåŠŸæˆ–ä¸æ˜¯é™æµé”™è¯¯ï¼Œç›´æ¥è¿”å›
            if response.status_code == 200 or not is_rate_limit_error(response):
                if attempt > 0:
                    logger.info(f"ğŸ”¥ {req_prefix} é‡è¯•æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡)")
                return response
            
            # æ£€æµ‹åˆ°é™æµé”™è¯¯
            if attempt < max_retries:
                # æŒ‡æ•°é€€é¿ï¼š2^attempt ç§’ï¼ŒåŠ ä¸ŠéšæœºæŠ–åŠ¨
                base_wait = 2 ** attempt
                jitter = random.uniform(0.1, 0.5)  # éšæœºæŠ–åŠ¨é¿å…åŒæ—¶é‡è¯•
                wait_time = min(base_wait + jitter, 30)  # æœ€å¤§ç­‰å¾…30ç§’
                
                logger.warning(f"ğŸ”¥ {req_prefix} æ£€æµ‹åˆ°é™æµ (ç¬¬{attempt + 1}æ¬¡)ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.error(f"ğŸ”¥ {req_prefix} é™æµé‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ ({max_retries} æ¬¡)")
                
        except Exception as e:
            if attempt < max_retries:
                wait_time = 2 ** attempt
                logger.warning(f"ğŸ”¥ {req_prefix} APIè°ƒç”¨å¼‚å¸¸ (ç¬¬{attempt + 1}æ¬¡): {str(e)}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.error(f"ğŸ”¥ {req_prefix} APIè°ƒç”¨é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™: {str(e)}")
                raise
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åä¸€æ¬¡çš„å“åº”
    return response

@app.route('/v1/messages', methods=['POST'])
def messages():
    """å¤„ç†Anthropicæ ¼å¼çš„æ¶ˆæ¯è¯·æ±‚ï¼ˆå…¼å®¹Claude APIï¼‰"""
    # ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
    request_id = str(uuid.uuid4())[:8]
    request_start_time = time.time()
    
    try:
        # è·å–Anthropicæ ¼å¼çš„è¯·æ±‚
        anthropic_request = request.get_json()
        model = anthropic_request.get('model', 'claude-3-sonnet-20240229')
        
        logger.info(f"ğŸ”¥ [REQ:{request_id}] æ–°HTTPè¯·æ±‚å¼€å§‹ - æ¨¡å‹: {model}")
        logger.info(f"ğŸ”¥ [REQ:{request_id}] å®¢æˆ·ç«¯IP: {request.remote_addr}")
        logger.info(f"ğŸ”¥ [REQ:{request_id}] User-Agent: {request.headers.get('User-Agent', 'Unknown')}")
        
        # ğŸ”¥ è¯·æ±‚å»é‡æ£€æŸ¥ - é˜²æ­¢Claude Codeé‡å¤è¯·æ±‚
        is_duplicate, cached_response = request_deduplicator.is_duplicate_request(anthropic_request)
        if is_duplicate and cached_response:
            logger.info(f"ğŸ”„ [REQ:{request_id}] è¿”å›ç¼“å­˜å“åº”ï¼Œé¿å…é‡å¤å¤„ç†")
            # è®¡ç®—ç¼“å­˜å“åº”æ—¶é—´
            request_duration = time.time() - request_start_time
            logger.info(f"ğŸ”„ [REQ:{request_id}] âœ… ç¼“å­˜è¯·æ±‚å®Œæˆ - æ€»è€—æ—¶: {request_duration:.2f}ç§’")
            return jsonify(cached_response)
        
        logger.info(f"ğŸ”¥ [REQ:{request_id}] è¯·æ±‚è¯¦æƒ…: {anthropic_request}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if 'messages' not in anthropic_request:
            logger.warning("è¯·æ±‚ç¼ºå°‘messageså­—æ®µ")
            return jsonify({
                'type': 'error',
                'error': {
                    'type': 'invalid_request_error',
                    'message': 'Missing required field: messages'
                }
            }), 400
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        logger.info(f"ğŸ”¥ [REQ:{request_id}] è½¬æ¢è¯·æ±‚æ ¼å¼ä¸ºOpenAIæ ¼å¼")
        openai_request = converter.convert_request(anthropic_request)
        
        # è°ƒç”¨OpenAI API
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {get_openai_config()["api_key"]}'
        }
        
        logger.info(f"ğŸ”¥ [REQ:{request_id}] è°ƒç”¨OpenAI API: {get_openai_config()['base_url']}/chat/completions")
        
        # ä½¿ç”¨æ™ºèƒ½é™æµå¤„ç†
        def make_api_call():
            logger.info(f"ğŸ”¥ [REQ:{request_id}] å‘èµ·OpenAI APIè°ƒç”¨")
            return http_session.post(
                f'{get_openai_config()["base_url"]}/chat/completions',
                headers=headers,
                json=openai_request,
                timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
        
        response = handle_rate_limit_with_backoff(make_api_call, request_id=request_id)
        
        if response.status_code == 200:
            logger.info(f"ğŸ”¥ [REQ:{request_id}] OpenAI APIè°ƒç”¨æˆåŠŸ")
            openai_response = response.json()
            logger.info(f"ğŸ”¥ [REQ:{request_id}] OpenAIåŸå§‹å“åº”: {str(openai_response)[:500]}...")
            
            # è½¬æ¢ä¸ºAnthropicæ ¼å¼
            anthropic_response = converter.convert_response(openai_response)
            anthropic_response['model'] = model
            
            # ğŸ”´ æ™ºèƒ½ä¿®å¤å·²ç¦ç”¨ - è¿™æ˜¯å¯¼è‡´Claude Codeå¾ªç¯çš„æ ¹æœ¬åŸå› 
            # æ™ºèƒ½ä¿®å¤è¿‡åº¦å¹²é¢„äº†æ­£å¸¸çš„å“åº”æµç¨‹ï¼Œå¯¼è‡´å·¥å…·è°ƒç”¨å‚æ•°é”™è¯¯
            # ä¿ç•™åŸå§‹å“åº”ï¼Œè®©Claude Codeæ­£å¸¸å¤„ç†
            logger.info(f"ğŸ”¥ [REQ:{request_id}] âš ï¸ æ™ºèƒ½ä¿®å¤å·²ç¦ç”¨ - ä½¿ç”¨åŸå§‹å“åº”")
            
            # smart_converter = SmartConverter()
            # fixed_response = smart_converter.fix_response_if_needed(anthropic_response, anthropic_request)
            # 
            # if fixed_response != anthropic_response:
            #     logger.info("ğŸ”§ æ™ºèƒ½ä¿®å¤å·²åº”ç”¨")
            #     anthropic_response = fixed_response
            
            logger.info(f"ğŸ”¥ [REQ:{request_id}] è½¬æ¢åAnthropicå“åº”: {str(anthropic_response)[:500]}...")
            
            # ğŸ”¥ ç¼“å­˜å“åº”ä»¥é˜²æ­¢é‡å¤è¯·æ±‚
            request_deduplicator.cache_response(anthropic_request, anthropic_response)
            
            # è®¡ç®—è¯·æ±‚å¤„ç†æ—¶é—´
            request_duration = time.time() - request_start_time
            logger.info(f"ğŸ”¥ [REQ:{request_id}] âœ… è¯·æ±‚å®Œæˆ - æ€»è€—æ—¶: {request_duration:.2f}ç§’")
            
            return jsonify(anthropic_response)
        else:
            error_msg = f'OpenAI API error: {response.text}'
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
            if response.status_code == 429 or 'TPM' in response.text or 'RPM' in response.text:
                logger.warning("æ£€æµ‹åˆ°APIé™æµï¼Œè¿”å›é™æµé”™è¯¯å“åº”")
                return jsonify({
                    'type': 'error',
                    'error': {
                        'type': 'rate_limit_error',
                        'message': 'API rate limit exceeded. Please try again in a few moments.'
                    }
                }), 429
            else:
                return jsonify({
                    'type': 'error',
                    'error': {
                        'type': 'api_error',
                        'message': error_msg
                    }
                }), response.status_code
            
    except Exception as e:
        error_msg = f'Conversion error: {str(e)}'
        logger.error(f"è½¬æ¢é”™è¯¯: {str(e)}", exc_info=True)
        return jsonify({
            'type': 'error',
            'error': {
                'type': 'conversion_error',
                'message': error_msg
            }
        }), 500

@app.route('/messages', methods=['POST'])
def messages_anthropic():
    """æ ‡å‡†Anthropic APIç«¯ç‚¹"""
    return messages()

@app.route('/v1/complete', methods=['POST'])
def complete():
    """å¤„ç†Anthropicæ ¼å¼çš„å®Œæˆè¯·æ±‚ï¼ˆå…¼å®¹æ—§ç‰ˆAPIï¼‰"""
    try:
        # è·å–Anthropicæ ¼å¼çš„è¯·æ±‚
        anthropic_request = request.get_json()
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if 'prompt' not in anthropic_request:
            return jsonify({
                'type': 'error',
                'error': {
                    'type': 'invalid_request_error',
                    'message': 'Missing required field: prompt'
                }
            }), 400
        
        # è½¬æ¢promptä¸ºmessagesæ ¼å¼
        prompt = anthropic_request.get('prompt', '')
        messages = [{"role": "user", "content": prompt}]
        
        # æ„å»ºæ–°çš„è¯·æ±‚
        new_request = {
            "model": anthropic_request.get('model', 'claude-3-sonnet-20240229'),
            "messages": messages,
            "max_tokens": anthropic_request.get('max_tokens', 1000),
            "temperature": anthropic_request.get('temperature', 1.0),
            "stream": anthropic_request.get('stream', False)
        }
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        openai_request = converter.convert_request(new_request)
        
        # è°ƒç”¨OpenAI API
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {get_openai_config()["api_key"]}'
        }
        
        response = http_session.post(
            f'{get_openai_config()["base_url"]}/chat/completions',
            headers=headers,
            json=openai_request,
            timeout=30
        )
        
        if response.status_code == 200:
            openai_response = response.json()
            # è½¬æ¢ä¸ºAnthropicæ ¼å¼
            anthropic_response = converter.convert_response(openai_response)
            anthropic_response['model'] = anthropic_request.get('model', 'claude-3-sonnet-20240229')
            return jsonify(anthropic_response)
        else:
            error_msg = f'OpenAI API error: {response.text}'
            app.logger.error(error_msg)
            return jsonify({
                'type': 'error',
                'error': {
                    'type': 'api_error',
                    'message': error_msg
                }
            }), response.status_code
            
    except Exception as e:
        error_msg = f'Conversion error: {str(e)}'
        app.logger.error(error_msg)
        return jsonify({
            'type': 'error',
            'error': {
                'type': 'conversion_error',
                'message': error_msg
            }
        }), 500

@app.route('/complete', methods=['POST'])
def complete_anthropic():
    """æ ‡å‡†Anthropicå®Œæˆç«¯ç‚¹"""
    return complete()

@app.route('/v1/models', methods=['GET'])
def list_models():
    """è¿”å›æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆAnthropicæ ¼å¼ï¼‰"""
    logger.info("è·å–æ¨¡å‹åˆ—è¡¨")
    
    # å°è¯•ä»ç›®æ ‡APIè·å–æ¨¡å‹åˆ—è¡¨
    try:
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {get_openai_config()["api_key"]}'
        }
        
        response = http_session.get(
            f'{get_openai_config()["base_url"]}/models',
            headers=headers,
            timeout=10
        )
        
        if response.status_code == 200:
            models_data = response.json()
            logger.info(f"ä»ç›®æ ‡APIè·å–åˆ° {len(models_data.get('data', []))} ä¸ªæ¨¡å‹")
            return jsonify(models_data)
        else:
            logger.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        logger.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {str(e)}")
    
    # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›é»˜è®¤çš„Anthropicæ¨¡å‹åˆ—è¡¨
    default_models = [
        "claude-3-5-sonnet-20241022",
        "claude-3-5-haiku-20241022",
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307"
    ]
    
    models_data = []
    for model in default_models:
        models_data.append({
            "id": model,
            "object": "model",
            "created": 1707879684,
            "owned_by": "anthropic"
        })
    
    logger.info(f"è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨ {len(models_data)} ä¸ªæ¨¡å‹")
    return jsonify({
        "object": "list",
        "data": models_data
    })

@app.route('/v1/messages/count_tokens', methods=['POST'])
def count_tokens():
    """è®¡ç®—tokenæ•°é‡ï¼ˆAnthropic APIå…¼å®¹ï¼‰"""
    try:
        logger.info("æ”¶åˆ°tokenè®¡ç®—è¯·æ±‚")
        anthropic_request = request.get_json()
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if 'messages' not in anthropic_request and 'text' not in anthropic_request:
            return jsonify({
                'type': 'error',
                'error': {
                    'type': 'invalid_request_error',
                    'message': 'Missing required field: messages or text'
                }
            }), 400
        
        # ç®€å•çš„tokenä¼°ç®—ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨tiktokenï¼‰
        text = ""
        if 'messages' in anthropic_request:
            for message in anthropic_request['messages']:
                if isinstance(message.get('content'), str):
                    text += message['content']
                elif isinstance(message.get('content'), list):
                    for content in message['content']:
                        if content.get('type') == 'text':
                            text += content.get('text', '')
        elif 'text' in anthropic_request:
            text = anthropic_request['text']
        
        # ç®€å•ä¼°ç®—ï¼šå¤§çº¦4ä¸ªå­—ç¬¦ = 1ä¸ªtoken
        estimated_tokens = max(1, len(text) // 4)
        
        logger.info(f"ä¼°ç®—tokenæ•°é‡: {estimated_tokens} (æ–‡æœ¬é•¿åº¦: {len(text)})")
        
        return jsonify({
            "input_tokens": estimated_tokens
        })
        
    except Exception as e:
        error_msg = f'Tokenè®¡ç®—é”™è¯¯: {str(e)}'
        logger.error(error_msg)
        return jsonify({
            'type': 'error',
            'error': {
                'type': 'token_calculation_error',
                'message': error_msg
            }
        }), 500

@app.route('/messages/count_tokens', methods=['POST'])
def count_tokens_anthropic():
    """æ ‡å‡†Anthropic tokenè®¡ç®—ç«¯ç‚¹"""
    return count_tokens()

@app.route('/config', methods=['GET'])
def get_config():
    """è·å–å½“å‰é…ç½®"""
    return jsonify(config_manager.config)

@app.route('/config', methods=['POST'])
def update_config():
    """æ›´æ–°é…ç½®"""
    try:
        new_config = request.get_json()
        
        # æ›´æ–°OpenAIé…ç½®
        if 'openai' in new_config:
            openai_cfg = new_config['openai']
            config_manager.update_openai_config(
                api_key=openai_cfg.get('api_key'),
                base_url=openai_cfg.get('base_url')
            )
        
        # æ›´æ–°æœåŠ¡å™¨é…ç½®
        if 'server' in new_config:
            server_cfg = new_config['server']
            config_manager.update_server_config(
                host=server_cfg.get('host'),
                port=server_cfg.get('port'),
                debug=server_cfg.get('debug')
            )
        
        
        # ä¿å­˜é…ç½®
        if config_manager.save_config():
            refresh_configs()  # åˆ·æ–°é…ç½®ç¼“å­˜
            return jsonify({"status": "success", "message": "é…ç½®å·²æ›´æ–°"})
        else:
            return jsonify({"status": "error", "message": "ä¿å­˜é…ç½®å¤±è´¥"}), 500
            
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 400

@app.route('/get-models', methods=['POST'])
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        data = request.get_json()
        url = data.get('url', '')
        key = data.get('key', '')
        
        if not url or not key:
            return jsonify({"models": []}), 200
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {key}'
        }
        
        models_response = requests.get(
            f'{url}/models',
            headers=headers,
            timeout=10
        )
        
        available_models = []
        if models_response.status_code == 200:
            try:
                models_data = models_response.json()
                if 'data' in models_data:
                    available_models = [model.get('id', '') for model in models_data['data'] if model.get('id')]
            except:
                pass
        
        return jsonify({"models": available_models}), 200
            
    except requests.exceptions.Timeout:
        return jsonify({"models": []}), 200
    except requests.exceptions.ConnectionError:
        return jsonify({"models": []}), 200
    except Exception as e:
        return jsonify({"models": []}), 200

@app.route('/test-connection', methods=['POST'])
def test_connection():
    """æµ‹è¯•OpenAI APIè¿æ¥"""
    try:
        data = request.get_json()
        url = data.get('url', '')
        key = data.get('key', '')
        test_model = data.get('model', '')
        
        if not url or not key:
            return jsonify({"success": False, "error": "URLå’Œå¯†é’¥ä¸èƒ½ä¸ºç©º"}), 400
        
        if not test_model:
            return jsonify({"success": False, "error": "è¯·é€‰æ‹©æˆ–è¾“å…¥è¦æµ‹è¯•çš„æ¨¡å‹"}), 400
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {key}'
        }
        
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°è¿›è¡Œæµ‹è¯•
        test_request = {
            "model": test_model,
            "messages": [
                {"role": "user", "content": "Hello, this is a test message."}
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        chat_response = requests.post(
            f'{url}/chat/completions',
            headers=headers,
            json=test_request,
            timeout=30
        )
        
        if chat_response.status_code == 200:
            return jsonify({
                "success": True, 
                "message": f"è¿æ¥æˆåŠŸï¼æ¨¡å‹ {test_model} æµ‹è¯•é€šè¿‡"
            })
        else:
            return jsonify({
                "success": False, 
                "error": f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {chat_response.status_code} - {chat_response.text}"
            }), 400
            
    except requests.exceptions.Timeout:
        return jsonify({"success": False, "error": "è¿æ¥è¶…æ—¶"}), 400
    except requests.exceptions.ConnectionError:
        return jsonify({"success": False, "error": "æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨"}), 400
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 400

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """OpenAIæ ¼å¼çš„èŠå¤©å®Œæˆç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    try:
        # è·å–OpenAIæ ¼å¼çš„è¯·æ±‚
        openai_request = request.get_json()
        model = openai_request.get('model', 'gpt-4')
        
        logger.info(f"æ”¶åˆ°OpenAIæ ¼å¼èŠå¤©å®Œæˆè¯·æ±‚ - æ¨¡å‹: {model}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if 'messages' not in openai_request:
            logger.warning("è¯·æ±‚ç¼ºå°‘messageså­—æ®µ")
            return jsonify({
                'error': {
                    'message': 'Missing required field: messages',
                    'type': 'invalid_request_error'
                }
            }), 400
        
        # ç›´æ¥è°ƒç”¨OpenAI API
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {get_openai_config()["api_key"]}'
        }
        
        logger.info(f"è°ƒç”¨OpenAI API: {get_openai_config()['base_url']}/chat/completions")
        
        # ä½¿ç”¨æ™ºèƒ½é™æµå¤„ç†
        def make_api_call():
            return http_session.post(
                f'{get_openai_config()["base_url"]}/chat/completions',
                headers=headers,
                json=openai_request,
                timeout=60
            )
        
        response = handle_rate_limit_with_backoff(make_api_call)
        
        if response.status_code == 200:
            logger.info("OpenAI APIè°ƒç”¨æˆåŠŸ")
            return jsonify(response.json())
        else:
            error_msg = f'OpenAI API error: {response.text}'
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return jsonify({
                'error': {
                    'message': error_msg,
                    'type': 'api_error'
                }
            }), response.status_code
            
    except Exception as e:
        error_msg = f'Chat completion error: {str(e)}'
        logger.error(f"èŠå¤©å®Œæˆé”™è¯¯: {str(e)}", exc_info=True)
        return jsonify({
            'error': {
                'message': error_msg,
                'type': 'server_error'
            }
        }), 500

@app.route('/v1/engines', methods=['GET'])
def list_engines():
    """è¿”å›æ”¯æŒçš„å¼•æ“åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼å…¼å®¹ï¼‰"""
    return list_models()

@app.route('/v1/files', methods=['GET', 'POST'])
def files():
    """æ–‡ä»¶æ“ä½œç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    if request.method == 'GET':
        # è¿”å›ç©ºæ–‡ä»¶åˆ—è¡¨
        return jsonify({
            "object": "list",
            "data": []
        })
    elif request.method == 'POST':
        # æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ 
        return jsonify({
            "id": "file_" + str(int(time.time())),
            "object": "file",
            "bytes": 0,
            "created_at": int(time.time()),
            "filename": "uploaded_file.txt",
            "purpose": "assistants"
        })

@app.route('/v1/assistants', methods=['GET', 'POST'])
def assistants():
    """åŠ©æ‰‹æ“ä½œç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    if request.method == 'GET':
        # è¿”å›ç©ºåŠ©æ‰‹åˆ—è¡¨
        return jsonify({
            "object": "list",
            "data": []
        })
    elif request.method == 'POST':
        # æ¨¡æ‹Ÿåˆ›å»ºåŠ©æ‰‹
        return jsonify({
            "id": "asst_" + str(int(time.time())),
            "object": "assistant",
            "created_at": int(time.time()),
            "name": "Claude Code Assistant",
            "model": "claude-3-sonnet-20240229",
            "instructions": "You are a helpful assistant for code development."
        })

@app.route('/v1/runs', methods=['GET', 'POST'])
def runs():
    """è¿è¡Œæ“ä½œç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    if request.method == 'GET':
        # è¿”å›ç©ºè¿è¡Œåˆ—è¡¨
        return jsonify({
            "object": "list",
            "data": []
        })
    elif request.method == 'POST':
        # æ¨¡æ‹Ÿåˆ›å»ºè¿è¡Œ
        return jsonify({
            "id": "run_" + str(int(time.time())),
            "object": "run",
            "created_at": int(time.time()),
            "assistant_id": "asst_demo",
            "status": "queued",
            "model": "claude-3-sonnet-20240229"
        })

@app.route('/v1/threads', methods=['GET', 'POST'])
def threads():
    """çº¿ç¨‹æ“ä½œç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    if request.method == 'GET':
        # è¿”å›ç©ºçº¿ç¨‹åˆ—è¡¨
        return jsonify({
            "object": "list",
            "data": []
        })
    elif request.method == 'POST':
        # æ¨¡æ‹Ÿåˆ›å»ºçº¿ç¨‹
        return jsonify({
            "id": "thread_" + str(int(time.time())),
            "object": "thread",
            "created_at": int(time.time()),
            "metadata": {}
        })

@app.route('/v1/messages/batches', methods=['GET', 'POST'])
def message_batches():
    """æ¶ˆæ¯æ‰¹å¤„ç†ç«¯ç‚¹ï¼ˆClaude Codeå…¼å®¹ï¼‰"""
    if request.method == 'GET':
        # è¿”å›ç©ºæ‰¹å¤„ç†åˆ—è¡¨
        return jsonify({
            "object": "list",
            "data": []
        })
    elif request.method == 'POST':
        # æ¨¡æ‹Ÿåˆ›å»ºæ‰¹å¤„ç†
        return jsonify({
            "id": "batch_" + str(int(time.time())),
            "object": "batch",
            "endpoint": "/v1/chat/completions",
            "completion_window": "24h",
            "status": "in_progress"
        })

@app.route('/', methods=['GET'])
def index():
    """è¿”å›é…ç½®é¡µé¢"""
    try:
        with open('config.html', 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return jsonify({"error": "é…ç½®é¡µé¢æœªæ‰¾åˆ°"}), 404

@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        "status": "healthy",
        "timestamp": int(time.time()),
        "version": "1.0.0"
    })

if __name__ == '__main__':
    # ä½¿ç”¨é…ç½®ä¸­çš„æœåŠ¡å™¨è®¾ç½®
    host = get_server_config().get('host', '0.0.0.0')
    port = get_server_config().get('port', 8080)
    debug = get_server_config().get('debug', True)
    
    # å•å®ä¾‹æ£€æŸ¥
    instance_checker = SingleInstanceChecker(port)
    
    if not instance_checker.check_single_instance():
        print("å•å®ä¾‹æ£€æŸ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)
    
    print(f"å¯åŠ¨APIè½¬æ¢æœåŠ¡å™¨...")
    print(f"é…ç½®é¡µé¢: http://{host}:{port}/")
    print(f"APIç«¯ç‚¹: http://{host}:{port}/v1/")
    print(f"é…ç½®API: http://{host}:{port}/config")
    
    # ç¦ç”¨Flaskçš„è‡ªåŠ¨é‡å¯ä»¥é¿å…å¤šå®ä¾‹é—®é¢˜
    app.run(host=host, port=port, debug=False, use_reloader=False)
